{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Based Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Topic Based Recommender\n",
    "1. Represent articles in terms of Topic Vector\n",
    "2. Represent user in terms of Topic Vector of read articles\n",
    "3. Calculate cosine similarity between read and unread articles \n",
    "4. Get the recommended articles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describing parameters**:\n",
    "\n",
    "*1. PATH_ARTICLE_TOPIC_DISTRIBUTION: specify the path where 'ARTICLE_TOPIC_DISTRIBUTION.csv' is present.* <br/>\n",
    "*2. PATH_NEWS_ARTICLES: specify the path where news_article.csv is present*  <br/>\n",
    "*3. NO_OF_TOPIC: Number of topics specified when training your topic model. This would refer to the dimension of        each vector representing an article*  <br/>\n",
    "*4. ARTICLES_READ: List of Article_Ids read by the user*  <br/>\n",
    "*5. NO_RECOMMENDED_ARTICLES: Refers to the number of recommended articles as a result*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_ARTICLE_TOPIC_DISTRIBUTION = \"/Users/sourabhrohilla/Downloads/Final/python/Topic Model/model/Article_Topic_Distribution.csv\"\n",
    "PATH_NEWS_ARTICLES = \"/Users/sourabhrohilla/Downloads/Final/news_articles.csv\"\n",
    "NO_OF_TOPICS=150\n",
    "ARTICLES_READ=[2,7]\n",
    "NUM_RECOMMENDED_ARTICLES=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Represent Read Article in terms of Topic Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22186, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_topic_distribution = pd.read_csv(PATH_ARTICLE_TOPIC_DISTRIBUTION)\n",
    "article_topic_distribution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article_Id</th>\n",
       "      <th>Topic_Id</th>\n",
       "      <th>Topic_Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.324485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.131476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>0.535940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.306691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>0.277037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Article_Id  Topic_Id  Topic_Weight\n",
       "0           0        25      0.324485\n",
       "1           0        27      0.131476\n",
       "2           0       127      0.535940\n",
       "3           1         5      0.306691\n",
       "4           1        47      0.277037"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_topic_distribution.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Generate Article-Topic Distribution matrix ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4831, 150)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pivot the dataframe\n",
    "article_topic_pivot = article_topic_distribution.pivot(index='Article_Id', columns='Topic_Id', values='Topic_Weight')\n",
    "#Fill NaN with 0\n",
    "article_topic_pivot.fillna(value=0, inplace=True)\n",
    "#Get the values in dataframe as matrix\n",
    "articles_topic_matrix = article_topic_pivot.values\n",
    "articles_topic_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Topic_Id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article_Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.306691</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.077002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.396528</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Topic_Id    0    1    2         3    4         5    6    7    8    9   ...   \\\n",
       "Article_Id                                                             ...    \n",
       "0           0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  0.0  0.0 ...    \n",
       "1           0.0  0.0  0.0  0.000000  0.0  0.306691  0.0  0.0  0.0  0.0 ...    \n",
       "2           0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  0.0  0.0 ...    \n",
       "3           0.0  0.0  0.0  0.015589  0.0  0.077002  0.0  0.0  0.0  0.0 ...    \n",
       "4           0.0  0.0  0.0  0.000000  0.0  0.396528  0.0  0.0  0.0  0.0 ...    \n",
       "\n",
       "Topic_Id    140  141  142  143  144  145  146  147  148  149  \n",
       "Article_Id                                                    \n",
       "0           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 150 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_topic_pivot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Represent user in terms of Topic Vector of read articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***A user vector is represented in terms of average of read articles topic vector***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select user in terms of read article topic distribution\n",
    "row_idx = np.array(ARTICLES_READ)\n",
    "read_articles_topic_matrix=articles_topic_matrix[row_idx[:, None]]\n",
    "#Calculate the average of read articles topic vector \n",
    "user_vector = np.mean(read_articles_topic_matrix, axis=0)\n",
    "user_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate cosine similarity between read and unread articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(articles_topic_matrix, user_vector):\n",
    "    articles_similarity_score=cosine_similarity(articles_topic_matrix, user_vector)\n",
    "    recommended_articles_id = articles_similarity_score.flatten().argsort()[::-1]\n",
    "    #Remove read articles from recommendations\n",
    "    final_recommended_articles_id = [article_id for article_id in recommended_articles_id \n",
    "                                     if article_id not in ARTICLES_READ ][:NUM_RECOMMENDED_ARTICLES]\n",
    "    return final_recommended_articles_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2843, 3419, 2760, 3123, 3307]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_articles_id = calculate_cosine_similarity(articles_topic_matrix, user_vector)\n",
    "recommended_articles_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Recommendation Using Topic Model:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles Read\n",
      "[ 'US  South Korea begin joint military drill amid nuclear threat from North Korea'\n",
      " 'Dialogue crucial in finding permanent solution to Kashmir s crisis  PM Modi']\n",
      "\n",
      "\n",
      "Recommender \n",
      "[ 'Rajnath Singh s security is Pak s responsibility during SAARC visit  says Rijiju after JuD  Hizbul threats'\n",
      " 'Siachen avalanche  Indian Army says missing soldiers presumed dead'\n",
      " 'Military Plane Crashes Outside Seville Airport in Spain'\n",
      " 'Europe survives  year of hell   but worse expected to come in 2016'\n",
      " 'Jammu   Kashmir  Army Indicts 9 Soldiers for Killing 2 Kashmiri Youths in Budgam']\n"
     ]
    }
   ],
   "source": [
    "#Recommended Articles and their title\n",
    "news_articles = pd.read_csv(PATH_NEWS_ARTICLES)\n",
    "print 'Articles Read'\n",
    "print news_articles.loc[news_articles['Article_Id'].isin(ARTICLES_READ)]['Title'].values\n",
    "print '\\n'\n",
    "print 'Recommender '\n",
    "print news_articles.loc[news_articles['Article_Id'].isin(recommended_articles_id)]['Title'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Topics + NER Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic + NER Based Recommender\n",
    "\n",
    "1. Represent user in terms of - <br/>\n",
    "        (Alpha) <Topic Vector> + (1-Alpha) <NER Vector> <br/>\n",
    "   where <br/>\n",
    "   Alpha => [0,1] <br/>\n",
    "   [Topic Vector] => Topic vector representation of concatenated read articles <br/>\n",
    "   [NER Vector]   => Topic vector representation of NERs associated with concatenated read articles <br/>\n",
    "2. Calculate cosine similarity between user vector and articles Topic matrix\n",
    "3. Get the recommended articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALPHA = 0.01\n",
    "DICTIONARY_PATH = \"/Users/sourabhrohilla/Downloads/Final/python/Topic Model/model/dictionary_of_words.p\"\n",
    "LDA_MODEL_PATH = \"/Users/sourabhrohilla/Downloads/Final/python/Topic Model/model/lda.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named gensim",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-5330f4c1f5f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msnowball\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named gensim"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.chunk import tree2conlltags\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pickle\n",
    "import gensim\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Represent User in terms of Topic Distribution and NER\n",
    "\n",
    "1. Represent user in terms of read article topic distribution\n",
    "2. Represent user in terms of NERs associated with read articles\n",
    "        2.1 Get NERs of read articles\n",
    "        2.2 Load LDA model\n",
    "        2.3 Get topic distribution for the concated NERs\n",
    "3. Generate user vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Represent user in terms of read article topic distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_idx = np.array(ARTICLES_READ)\n",
    "read_articles_topic_matrix=articles_topic_matrix[row_idx[:, None]]\n",
    "#Calculate the average of read articles topic vector \n",
    "user_topic_vector = np.mean(read_articles_topic_matrix, axis=0)\n",
    "user_topic_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Represent user in terms of NERs associated with read articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get NERs of read articles\n",
    "def get_ner(article):\n",
    "    ne_tree = ne_chunk(pos_tag(word_tokenize(article)))\n",
    "    iob_tagged = tree2conlltags(ne_tree)\n",
    "    ner_token = ' '.join([token for token,pos,ner_tag in iob_tagged if not ner_tag==u'O']) #Discarding tokens with 'Other' tag\n",
    "    return ner_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NERs of Read Article => United States South Korea North United Nations Security Council North Korea UN North Korea South Korea Ulchi Freedom Guardian Command North Korean Korean People Army Ulji Freedom Guardian KPA South Korea North London Seoul Kim Jong Un North Korean Narendra Modi Kashmir Modi Jammu Kashmir Modi Burhan Wani Omar Abdullah Abdullah National Conference Congress PCC CPI Tarigami Valley Modi Kashmir Jammu Kashmir\n"
     ]
    }
   ],
   "source": [
    "articles = news_articles['Content'].tolist()\n",
    "user_articles_ner = ' '.join([get_ner(articles[i]) for i in ARTICLES_READ])\n",
    "print \"NERs of Read Article =>\", user_articles_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tknzr = TweetTokenizer()\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text=re.sub('[^\\w_\\s-]', ' ', text)                                            #remove punctuation marks \n",
    "    return cleaned_text                                                                    #and other symbols \n",
    "\n",
    "def tokenize(text):\n",
    "    word = tknzr.tokenize(text)                                                             #tokenization\n",
    "    filtered_sentence = [w for w in word if not w.lower() in stop_words]                    #removing stop words\n",
    "    stemmed_filtered_tokens = [stemmer.stem(plural) for plural in filtered_sentence]        #stemming\n",
    "    tokens = [i for i in stemmed_filtered_tokens if i.isalpha() and len(i) not in [0, 1]]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Cleaning the article\n",
    "cleaned_text = clean_text(user_articles_ner)\n",
    "article_vocabulary = tokenize(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load model dictionary\n",
    "model_dictionary = pickle.load(open(DICTIONARY_PATH,\"rb\"))\n",
    "#Generate article maping using IDs associated with vocab\n",
    "corpus = [model_dictionary.doc2bow(text) for text in [article_vocabulary]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load LDA Model\n",
    "lda =  models.LdaModel.load(LDA_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(29, 0.12263313269087657),\n",
       " (62, 0.050353370951179081),\n",
       " (84, 0.15588838753218867),\n",
       " (127, 0.36080067044623093),\n",
       " (135, 0.29498052303560879)]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get topic distribution for the concated NERs\n",
    "article_topic_distribution=lda.get_document_topics(corpus[0])\n",
    "article_topic_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ner_vector =[0]*NO_OF_TOPICS\n",
    "for topic_id, topic_weight in article_topic_distribution:\n",
    "    user_ner_vector[topic_id]=topic_weight\n",
    "len(user_ner_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Generate user vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# User_Vector =>  (Alpha) [Topic Vector] + (1-Alpha) [NER Vector] \n",
    "alpha_topic_vector = [topic_weight*ALPHA for topic_weight in user_topic_vector]\n",
    "alpha_ner_vector = [ner*(1-ALPHA) for ner in user_ner_vector]\n",
    "\n",
    "user_vector = np.sum(zip(alpha_topic_vector,alpha_ner_vector))\n",
    "user_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Calculate cosine similarity between user vector and articles Topic matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phoenix/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2843, 3419, 2760, 3123, 3307]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_articles_id = calculate_cosine_similarity(articles_topic_matrix, user_vector)\n",
    "recommended_articles_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Get recommended articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles Read\n",
      "2    US  South Korea begin joint military drill ami...\n",
      "7    Dialogue crucial in finding permanent solution...\n",
      "Name: Title, dtype: object\n",
      "\n",
      "\n",
      "Recommender \n",
      "2760    Rajnath Singh s security is Pak s responsibili...\n",
      "2843    Siachen avalanche  Indian Army says missing so...\n",
      "3123    Military Plane Crashes Outside Seville Airport...\n",
      "3307    Europe survives  year of hell   but worse expe...\n",
      "3419    Jammu   Kashmir  Army Indicts 9 Soldiers for K...\n",
      "Name: Title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Recommended Articles and their title\n",
    "news_articles = pd.read_csv(PATH_NEWS_ARTICLES)\n",
    "print 'Articles Read'\n",
    "print news_articles.loc[news_articles['Article_Id'].isin(ARTICLES_READ)]['Title']\n",
    "print '\\n'\n",
    "print 'Recommender '\n",
    "print news_articles.loc[news_articles['Article_Id'].isin(recommended_articles_id)]['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
